{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import d\n",
    "from transformer import CTransformer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import numpy as np\n",
    "import random, tqdm, sys, math, gzip\n",
    "\n",
    "device = d()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "Define parameters for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "max_pool = True\n",
    "num_heads = 8\n",
    "depth = 6\n",
    "NUM_CLS = 2\n",
    "lr = 0.0001\n",
    "lr_warmup = 10_000\n",
    "num_epochs = 80\n",
    "gradient_clipping = 1.0\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data\n",
    "\n",
    "We will use IMDB reviews dataset for sentiment classification. Two labels:\n",
    "\n",
    "* Positive: 2\n",
    "\n",
    "* Negative: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "train_list = list(train_iter)\n",
    "test_list = list(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.datapipes.iter.sharding.ShardingFilterIterDataPipe"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'When I ordered this from Blockbuster\\'s website I had no idea that it would be as terrible as it was. Who knows? Maybe I\\'d forgotten to take my ADD meds that day. I do know that from the moment the cast drove up in their station wagon, donned in their late 70\\'s-style wide collars, bell-bottoms and feathered hair, I knew that this misplaced gem of the disco era was glory bound for the dumpster.<br /><br />The first foretelling of just how bad things were to be was the narration at the beginning, trying to explain what cosmic forces were at play to wreak havoc upon the universe, forcing polyester and porno-quality music on the would-be viewer. From the opening scene with the poorly-done effects to the \"monsters\" from another world and then the house which jumps from universe to universe was as achingly painful as watching an elementary school production of \\'The Vagina Monologues\\'.<br /><br />Throughout the film, the sure sign something was about to happen was when a small ship would appear. The \"ship\" was comprised suspiciously of what looked like old VCR and camcorder parts and would attack anyone in its path. Of course if moved slower than Bob Barker\\'s impacted bowels, but it had menacing pencil-thin armatures and the ability to cast a ominous green glow that could stop bullets and equipped with a laser capable of cutting through mere balsa wood in an hour or two (with some assistance).<br /><br />Moving on... As the weirdness and bell bottoms continue... We found out that they\\'re caught in a \"Space Time Warp\". How do we garner this little nugget of scientific information? Because the oldest male lead tells his son that, in a more or less off-the-cuff fashion, like reminiscing about \\'how you won the big game\\' over a cup of joe or an ice-cold bottle of refreshing Coca-Cola. Was pops a scientist? Nope, but he knew about horses and has apparently meddled as an amateur in string theory and Einstein\\'s theories.<br /><br />The recording I watched on DVD was almost bootleg quality. The sound was muddy and the transfer looked like it had been shot off a theater screen with the video recorder on a cell phone, other than that, it was really, really, really bad. (There\\'s not enough \\'really\\'s\\' to describe it, really).<br /><br />I know some out there love this movie and compare it to other cult classics. I never saw this film on its original release, but even back then I think I would\\'ve come to the same conclusion: bury this one quick.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our vocabulary: <span style=\"background-color: yellow;\">**[Code Pointer 1]**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100684"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenizer!\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text.lower())\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_list), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Ensure the vocab size fits the argument\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, every word has an associated index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String to index\n",
    "S2I = vocab.get_stoi()\n",
    "\n",
    "# Index to string\n",
    "I2S = vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'made'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I2S[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us preprocess our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipelines for text and label processing\n",
    "text_pipeline = lambda x: [vocab[token.lower()] for token in tokenizer(x)]  # Lowercase conversion here\n",
    "label_pipeline = lambda x: 1 if x == 2 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create dataloaders for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch, vocab, text_pipeline, label_pipeline):\n",
    "    label_list, text_list = [], []\n",
    "    for label, text in batch:\n",
    "        label_list.append(label_pipeline(label))\n",
    "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64).to(device)\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64).to(device) \n",
    "    text_list = pad_sequence(text_list, padding_value=vocab['<pad>'])\n",
    "    # Transpose the text_list to have batch size as the first dimension\n",
    "    text_list = text_list.transpose(0, 1)  # Switching from [seq_len, batch_size] to [batch_size, seq_len]\n",
    "    return label_list, text_list\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_list, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_batch(x, vocab, text_pipeline, label_pipeline))\n",
    "test_dataloader = DataLoader(test_list, batch_size=batch_size, shuffle=False, collate_fn=lambda x: collate_batch(x, vocab, text_pipeline, label_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look to a batch of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in train_dataloader:\n",
    "    sample_label = element[0]\n",
    "    sample_text  = element[1]\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 14,  21,  10,  ...,   1,   1,   1],\n",
       "        [ 13,   9, 152,  ...,   1,   1,   1],\n",
       "        [ 13,  97,   9,  ...,   1,   1,   1],\n",
       "        ...,\n",
       "        [ 72,   4,  39,  ...,   1,   1,   1],\n",
       "        [ 14,  21,  10,  ...,   1,   1,   1],\n",
       "        [  2,  70, 160,  ...,   1,   1,   1]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the max sequence lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- nr. of training examples 25000\n",
      "- nr. of test examples 25000\n",
      "- maximum sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "print(f'- nr. of training examples {len(train_list)}')\n",
    "print(f'- nr. of test examples {len(test_list)}')\n",
    "\n",
    "if max_length < 0.0:\n",
    "    max_seq_length = 0\n",
    "    for _, text_batch in train_dataloader:\n",
    "        # text_batch is a tuple of (label_list, text_list)\n",
    "        # Here, we're interested in text_list, which is the second item in the tuple\n",
    "        current_max = text_batch.shape[1]  # Get the sequence length dimension\n",
    "        if current_max > max_seq_length:\n",
    "            max_seq_length = current_max\n",
    "        \n",
    "    max_seq_length_doubled = max_seq_length * 2\n",
    "else:\n",
    "    max_seq_length = max_length\n",
    "\n",
    "print(f'- maximum sequence length: {max_seq_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and optimizer\n",
    "\n",
    "We create model and define optimizer.\n",
    "<span style=\"background-color: yellow;\">**[Code Pointer 6]**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = CTransformer(emb=embedding_size, heads=num_heads, depth=depth, seq_length=max_seq_length, num_tokens=vocab_size, num_classes=NUM_CLS, max_pool=max_pool)\n",
    "if torch.cuda.is_available():\n",
    "    model.to(device)\n",
    "\n",
    "opt = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "sch = torch.optim.lr_scheduler.LambdaLR(opt, lambda i: min(i / (lr_warmup / batch_size), 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [02:31<00:00,  1.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:57<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- test accuracy 0.665\n",
      "\n",
      " epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████████████████████████████████████████████████████                                              | 105/196 [01:23<01:12,  1.26it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m label_list, text_list\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create data loaders\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_list, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mcollate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_pipeline\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_list, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: collate_batch(x, vocab, text_pipeline, label_pipeline))\n",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m, in \u001b[0;36mcollate_batch\u001b[0;34m(batch, vocab, text_pipeline, label_pipeline)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, text \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m      4\u001b[0m     label_list\u001b[38;5;241m.\u001b[39mappend(label_pipeline(label))\n\u001b[0;32m----> 5\u001b[0m     processed_text \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     text_list\u001b[38;5;241m.\u001b[39mappend(processed_text)\n\u001b[1;32m      7\u001b[0m label_list \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(label_list, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\u001b[38;5;241m.\u001b[39mto(device) \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "seen = 0\n",
    "for e in range(num_epochs):\n",
    "\n",
    "    print(f'\\n epoch {e}')\n",
    "    model.train(True)\n",
    "\n",
    "    for batch in tqdm.tqdm(train_dataloader):\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        input = batch[1]\n",
    "        label = batch[0]\n",
    "\n",
    "        # Limit text lenght\n",
    "        if input.shape[1] > max_seq_length:\n",
    "            input = input[:, :max_seq_length]\n",
    "        out = model(input)\n",
    "\n",
    "        # Loss function (negative log likelihood)\n",
    "        loss = F.nll_loss(out, label)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # clip gradients\n",
    "        # - If the total gradient vector has a length > 1, we clip it back down to 1.\n",
    "        if gradient_clipping > 0.0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "\n",
    "        opt.step()\n",
    "        sch.step()\n",
    "\n",
    "        seen += input.size(0)\n",
    "        # tbw.add_scalar('classification/train-loss', float(loss.item()), seen)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.train(False)\n",
    "        tot, cor= 0.0, 0.0\n",
    "\n",
    "        for batch in tqdm.tqdm(test_dataloader):\n",
    "\n",
    "            input = batch[1]\n",
    "            label = batch[0]\n",
    "\n",
    "            if input.shape[1] > max_seq_length:\n",
    "                input = input[:, :max_seq_length]\n",
    "\n",
    "            out = model(input).argmax(dim=1)\n",
    "\n",
    "            tot += float(input.shape[0])\n",
    "            cor += float((label == out).sum().item())\n",
    "\n",
    "        acc = cor / tot\n",
    "        print(f'-- test accuracy {acc:.3}')\n",
    "        # tbw.add_scalar('classification/test-loss', float(loss.item()), e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = torch.load(\"sent-model-27-epochs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1776e-07, 1.0000e+00]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "text = \"This is the bed reading group I have ever attended!\"\n",
    "processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64).to(device)\n",
    "processed_text = torch.unsqueeze(processed_text, 0)\n",
    "if processed_text.shape[1] > max_seq_length:\n",
    "    processed_text = processed_text[:, :max_seq_length]\n",
    "\n",
    "# evaluate model:\n",
    "model_new.eval()\n",
    "with torch.no_grad():\n",
    "    probs = torch.exp(model_new(processed_text))\n",
    "\n",
    "print(probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
